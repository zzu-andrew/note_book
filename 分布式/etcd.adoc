

:toc:

// 保证所有的目录层级都可以正常显示图片
:path: 分布式/
:imagesdir: ../image/

// 只有book调用的时候才会走到这里
ifdef::rootpath[]
:imagesdir: {rootpath}{path}{imagesdir}

elif::rootpath[]
:rootpath: ../
endif::rootpath[]


== `ETCD`

===  特点
- 完全复制：集群中的每个节点都可以使用完整的存档
- 高可用性： `Etcd`可用于避免硬件的单点故障或网络问题
- 一致性：每次读取都会返回跨多主机的最新写入
- 简单：包括一个定义良好，面相用户的`API`(`gRPC`)
- 安全：实现了带有可选的客户端证书身份验证的自动化`TLS`
- 快速：每秒10000次写入的基准速度
- 可靠：使用`Raft`算法实现了强一致，高可用的服务存储目录


image::image-2022-07-27-10-44-47-608.png[]

> - HTTP server 用于处理用户发送的`API`请求
> - Store 用于处理`etcd`支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等
> - Raft 强一致算法的具体实现，是`etcd`核心
> - `WAL` Write ahead log 预写式日志，是`etcd`数据存储的形式，除了内存中存储的所有的数据的状态以及节点的索引以外，`etcd`就通过`WAL`进行持久化存储，`WAL`中所有的数据提交前都会事先记录日志，`snapshot`是为了防止数据过多而进行的状态快照，`Entry`表示存储的具体日志的内容。



=== 应用场景

==== 服务发现

服务发现是分布式系统中最常见的问题之一。即同一个分布式集群中的进程或者服务，要如何才能找到对方并建立连接。本质上来说，服务 发现就是想要了解集群中是否有进程在监听`UDP`或`TCP`端口，并且通过名字就能查找和连接。

==== 配置中心

将一些配置信息放置到`etcd`上进行集中管理。

这类场景的使用通常是，应用在启动的时候主动从`etcd`获取一次配置配置信息，同时，在`etcd`节点上注册一个`watcher`并等待，以后每次配置有更新的时候，`etcd`都会实时通知订阅者，以此达到获取最新配置信息的目的。

==== 分布式锁

因为`etcd`使用`raft`算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务具有两种使用方式，一是保持独占，二是控制时序。

- 保持独占：所有获取锁的用户最终只有一个可以得到
- 控制时序：所有想要得到锁的用户都会被执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序。

==== `etcd`集群

`etcd`作为一个高可用键值存储系统，天生就是为了集群化而设计的，由于`Raft`算法在做决策时需要多数节点的投票，所以`etcd`一般部署集群个数推荐奇数个节点。



==== `etcd`安装



==== 运行

1. 下载对应版本，直接双击进行运行就可以了。

2. `etcdctl  --endpoints=http://127.0.0.1:2379 put nazha "dsb"`  使用`etcdctl`进行连接和设置`kv`值

[source,bash]
----
andrew$ ./etcdctl --endpoints=http://127.0.0.1:2379 put baodelu "dsb"
OK
----



==== 使用`go`语言代码连接服务器

[source,go]
----
package main

import (
	"context"
	"fmt"
	"time"

	"go.etcd.io/etcd/clientv3"
)

var (
	dialTimeout    = 5 * time.Second
	requestTimeout = 4 * time.Second
	// 节点，可以传递多个节点的链接
	endpoints      = []string{"127.0.0.1:2379"}
)

func main() {

	cfg := clientv3.Config{
		Endpoints:   endpoints,
		DialTimeout: dialTimeout,
	}

	cli, err := clientv3.New(cfg)

	if err != nil {
		// handle error
		fmt.Printf("connect to etcd failed, err:%v\n", err)
		return
	}

	fmt.Println("connect to etcd success")

	defer cli.Close()

	// put
	ctx, cancel := context.WithTimeout(context.Background(), requestTimeout)
	_, err = cli.Put(ctx, "标", "hello")
	cancel()
	if err != nil {
		fmt.Printf("put to etcd failed, err:%v\n", err)
		return
	}

	// get
	ctx, cancel = context.WithTimeout(context.Background(), requestTimeout)
	resp, err := cli.Get(ctx, "标", clientv3.WithPrefix())
	cancel()

	if err != nil {
		fmt.Printf("get from etcd failed, err:%v\n", err)
		return
	}

	for _, kv := range resp.Kvs {
		fmt.Printf("%s:%s\n", kv.Key, kv.Value)
	}
}
----


[plantuml, target=produce-etcd, format=png]
----
producer -> etcd : 注册一个topic
consumer -> etcd : 订阅一个topic

producer -> etcd : 发布信息到topic
etcd -> consumer : 订阅topic的consumer接收到信息
----


=== Etcd中watch源码解析

etd是一个cs网络架构，源码分析时应该从client到server，watch主打一个监听功能，client主要来请求对key值的监听，并且接收key值的变更通知。server主要完成对client请求key值的监听，并在kv变动时及时通知client。

image::image-2023-05-18-14-10-46-501.png[]
image::image-2023-05-19-09-38-46-102.png[]


图上很复杂，但是实际实现很简单，我们看下client端的代码

[source, go]
----

type Watcher interface {
    // 可以实现对完整key值或者key前缀的监听，监听事件将会通过watch接口返回的WatchChan返回
    // 如果watch 修订版本提供的为止或者为0，WatchChan只会返回服务端接收到watch请求之后发生的事件
	// 如果context被取消，关联的watchChan也会关闭，如果watch不在使用ctx要及时关闭
	Watch(ctx context.Context, key string, opts ...OpOption) WatchChan

	// RequestProgress requests a progress notify response be sent in all watch channels.
	RequestProgress(ctx context.Context) error

	// Close closes the watcher and cancels all watch requests.
	Close() error
}

// Watcher interface 的实现
type watcher struct {
    // watch interface实现
	remote   pb.WatchClient
    // rpc 调用前设置 option
	callOpts []grpc.CallOption
    // 用于保护 streams map的操作安全
	mu sync.Mutex
	// 当前ctx所创建的所有watch stream流
	streams map[string]*watchGrpcStream
	lg      *zap.Logger
}

// watchGrpcStream 追踪所有绑定到一个grpc 流的watch
type watchGrpcStream struct {
    // watcher interface实现对象
	owner    *watcher
	remote   pb.WatchClient
	callOpts []grpc.CallOption

	// 用于控制 remote.Watch 请求，主要是ctx.Done() 用来停止对应的请求
	ctx context.Context
	// ctxKey is the key used when looking up this stream's context
    // @ streams holds all the active grpc streams keyed by ctx value.
    //	streams map[string]*watchGrpcStream
	ctxKey string
    // ctx 的cancel函数
	cancel context.CancelFunc
	// 同一个grpc上所有活跃的watchers
	substreams map[int64]*watcherStream
	// 当前grpc上所有恢复的watcher
	resuming []*watcherStream
	// reqc sends a watch request from Watch() to the main goroutine
	reqc chan watchStreamRequest
	// respc receives data from the watch client
	respc chan *pb.WatchResponse
	// donec closes to broadcast shutdown
	donec chan struct{}
	// errc transmits errors from grpc Recv to the watch stream reconnect logic
	errc chan error
	// closingc gets the watcherStream of closing watchers
	closingc chan *watcherStream
	// wg is Done when all substream goroutines have exited
	wg sync.WaitGroup
	// resumec closes to signal that all substreams should begin resuming
	resumec chan struct{}
	// closeErr is the error that closed the watch stream
	closeErr error
	lg *zap.Logger
}

// Watch 向run协程抛出一个 watch 请求并且等待一个watcher 通道(watch response)
func (w *watcher) Watch(ctx context.Context, key string, opts ...OpOption) WatchChan {
	// 按照前面注册的op 来构造一个Op对象
	ow := opWatch(key, opts...)

	var filters []pb.WatchCreateRequest_FilterType
	if ow.filterPut {
		filters = append(filters, pb.WatchCreateRequest_NOPUT)
	}
	if ow.filterDelete {
		filters = append(filters, pb.WatchCreateRequest_NODELETE)
	}
	// 构造一个request
	wr := &watchRequest{
		ctx:            ctx,
		createdNotify:  ow.createdNotify,
		key:            string(ow.key),
		end:            string(ow.end),
		rev:            ow.rev,
		progressNotify: ow.progressNotify,
		fragment:       ow.fragment,
		filters:        filters,
		prevKV:         ow.prevKV,
		retc:           make(chan chan WatchResponse, 1),
	}

	ok := false
	// 根据Ctx获取ctxKey，这样后面只要能拿到ctx就能获取对应的watchGrpcStream对象
	ctxKey := streamKeyFromCtx(ctx)

	var closeCh chan WatchResponse
	for {
		// find or allocate appropriate grpc watch stream
		w.mu.Lock()
		// streams holds all the active grpc streams keyed by ctx value.
		if w.streams == nil { // 如果streams没有初始化，认为watcher有问题，直接出错返回
			// closed
			w.mu.Unlock()
			ch := make(chan WatchResponse)
			close(ch)
			return ch
		}
		// 查看对应ctxKey是否已经存在对应的watcher,如果没有就新创建一个
		// 防止多次创建，下面有continue
		wgs := w.streams[ctxKey]
		if wgs == nil {
			wgs = w.newWatcherGrpcStream(ctx)
			w.streams[ctxKey] = wgs
		}
		donec := wgs.donec
		reqc := wgs.reqc
		w.mu.Unlock()

		// couldn't create channel; return closed channel
		if closeCh == nil {
			closeCh = make(chan WatchResponse, 1)
		}

		// submit request
		select {
		// 向主协程发送 watch request请求
		case reqc <- wr:
			ok = true
		//	 停止
		case <-wr.ctx.Done():
			ok = false
		case <-donec:
			ok = false
			if wgs.closeErr != nil {
				closeCh <- WatchResponse{Canceled: true, closeErr: wgs.closeErr}
				break
			}
			// retry; may have dropped stream from no ctxs
			continue
		}

		// receive channel
		if ok {
			select {
			case ret := <-wr.retc:
				return ret
			case <-ctx.Done():
			case <-donec:
				if wgs.closeErr != nil {
					closeCh <- WatchResponse{Canceled: true, closeErr: wgs.closeErr}
					break
				}
				// retry; may have dropped stream from no ctxs
				continue
			}
		}
		break
	}

	close(closeCh)
	return closeCh
}

----

整个watch过程如下：

1. 按照传入的OpOption构造watch使用的watchRequest
2. 添加时间过滤器
3. 添加新的grpc watch stream
4. 发送watch请求到reqc通道
5. 返回WatchResponse给接收chan的客户端










https://github.com/zzu-andrew/etcd-learning[etcd学习]