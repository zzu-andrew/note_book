

:toc:

// 保证所有的目录层级都可以正常显示图片
:path: C++知识点总结/
:imagesdir: ../image/

// 只有book调用的时候才会走到这里
ifdef::rootpath[]
:imagesdir: {rootpath}{path}{imagesdir}
endif::rootpath[]


== 高性能


=== placement new

placement new允许我们将对象创建于已经申请的内存中，因为placement new实际上并没有申请内存，所以也不存在所谓的placement delete

[source,cpp]
----
// PlacementNew
void PlacementNew() {
    char *buffer = new char[sizeof(MemoryManagement) * 3];

    auto *lp1 = new(buffer) MemoryManagement;
    auto *lp2 = new(buffer + sizeof(MemoryManagement)) MemoryManagement;
    auto *lp3 = new(buffer + sizeof(MemoryManagement) * 2) MemoryManagement;

    // 如果需要析枸，需要自己调用析枸函数
    lp1->~MemoryManagement();
    lp2->~MemoryManagement();
    lp3->~MemoryManagement();

    delete[] buffer;
}
----




=== SMP IRQ affinity

当一个硬件(如磁盘控制器或者以太网卡), 需要打断CPU的工作时, 它就触发一个中断. 该中断通知CPU发生了某些事情并且CPU应该放下当前的工作去处理这个事情. 为了防止多个设置发送相同的中断, Linux设计了一套中断请求系统, 使得计算机系统中的每个设备被分配了各自的中断号, 以确保它的中断请求的唯一性. 从2.4 内核开始, Linux改进了分配特定中断到指定的处理器(或处理器组)的功能. 这被称为SMP IRQ affinity, 它可以控制系统如何响应各种硬件事件. 允许你限制或者重新分配服务器的工作负载, 从而让服务器更有效的工作. 以网卡中断为例，在没有设置SMP IRQ affinity时， 所有网卡中断都关联到CPU0, 这导致了CPU0负载过高，而无法有效快速的处理网络数据包，导致了瓶颈。 通过SMP IRQ affinity， 把网卡多个中断分配到多个CPU上，可以分散CPU压力，提高数据处理速度。

==== 相关设置文件

- `/proc/irq/<irqnumber>/smp_affinity`

/proc/irq/IRQ#/smp_affinity 和 /proc/irq/IRQ#/smp_affinity_list 指定了哪些CPU能够关联到一个给定的IRQ源. 这两个文件包含了这些指定cpu的cpu位掩码(smp_affinity)和cpu列表(smp_affinity_list). 不允许关闭所有CPU， 同时如果IRQ控制器不支持中断请求亲和(IRQ affinity)，则这些设置的值将保持不变(既关联到所有CPU).

设置方法，按照bitmask进行设置，十六进制表示，转化为二进制之后，每一位代表一个CPU

[source, cpp]
----
           二进制  十六进制
   CPU 0   0001    1
   CPU 1   0010    2
   CPU 2   0100    4
   CPU 3   1000    8
----

假如有一个4个CPU的系统， 我们能给一个IRQ分配15种不同的CPU组合(实际上有16种，但我们不能给任何中断分配中断亲和为”0”的值， 即使你这么做，系统也会忽略你的做法)

[source, bash]
----
echo $bitmask > /proc/irq/IRQ#/smp_affinity

# 把44号中断绑定到前4个CPU上
echo f > /proc/irq/44/smp_affinity
----

- `/proc/irq/IRQ#/smp_affinity_list`

设置该文件取得的效果与/proc/irq/IRQ#/smp_affinity是一致的，它们两者是联动关系(既设置其中之一，另一个文件也随着改变), 有些系统可能没有该文件, 设置方法如下

[source, bash]
----
echo $cpuindex1-$cpuindex2 >/proc/irq/IRQ#/smp_affinity_list
# 将44号中断绑定到前4个CPU上(CPU0-3)上
echo 0-3 > /proc/irq/44/smp_affinity_list
----

- `/proc/irq/default_smp_affinity`

/proc/irq/default_smp_affinity 指定了默认情况下未激活的IRQ的中断亲和掩码(affinity mask).一旦IRQ被激活，它将被设置为默认的设置(即default_smp_affinity中记录的设置). 该文件能被修改. 默认设置是0xffffffff.

对于UDP测试在IRQ affinity上性能的下降， 查阅了内核源码(drivers/net/bnx2.c)及资料, bnx2 网卡的RSS hash不支持对UDP的端口进行计算，从而导致单独启用IRQ affinity的时候(这时候由硬件进行hash计算), UDP的数据只被hash了IP地址而导致数据包的转发出现集中在某个CPU的现象. 这是此次测试的局限所在，由于测试只是一台服务器端及一台客户端，所有UDP的IP地址都相同，无法体现UDP性能在单独启用IRQ affinity的性能提升




=== 如何更快的接收数据

物理网卡到内核的网络数据的接收流程

1. 驱动加载
2. 数据包到达网卡控制器(NIC)
3. 数据包被复制到内核空间(DMA -> ring_buffer)
4. 产生硬件中断，通知系统数据可读
5. 驱动调用NAPI激活poll函数，读取ring_buffer中的数据
6. ring_buffer对应的内存区域被解除映射(unmapped)
7. 数据包被封装为skb结构体，准备传递到上层协议站
8. 如果开启网卡多队列，数据帧会被负载到多个CPU上进行处理
9. 数据帧经过队列，递交上层协议栈

以上过程需要程序员重点关注的地方：

a. 数据包到达网卡
b. 网卡接收数据包时，需要产生一个软中断，告诉CPU数据已经到了，CPU调用ksoftirqd线程处理，中断事件
c. 内核接管，将数据从网卡中取出来，并将数据交给后面的内核协议栈处理

[plantuml, diagram=nic-queickdata, format=png]
.内核网络数据流程
....
@startuml
[*] -> packets
packets -> NIC : internet
NIC -> CPU : NAPI
CPU -> Kernel
Kernel -> skb
skb -> [*]
@enduml
....

网卡在收到数据时，都必须把数据从链路层拷贝到机器的内存里，而这块内存就是网卡在启动时，通过接口向内核、向操作系统申请而来的。内存一旦申请下来，地址确定之后，后续网卡在收到数据的时候，就可以直接通过 DMA 的机制，直接把数据包传送到内存固定的地址中去，甚至不需要 CPU 的参与。

image::nicpic.jpeg[nicpic]

到队列内存的分配可以看下上图，很早之前的网卡都是单队列的机制，但现代的网卡大多都是多队列的。好处就是机器网卡的数据接收可以被负载均衡到多个 CPU 上，因此会提供多个队列

image::../image/image-2022-06-26-17-45-04-444.png[CPUksoftirq]

NAPI是现代网络数据包处理框架中非常重要的一个扩展。之所以现在能支持 10G、20G、25G 等非常高速的网卡，NAPI 机制起到了非常大的作用。当然 NAPI 并不复杂，其核心就两点：中断、轮循。一般来说，网卡在接收数据时肯定是收一个包，产生一个中断，然后在中断处理函数的时候将包处理掉。处在收包、处理中断，下一个收包，再处理中断，这样的循环中。而 NAPI 机制优势在于只需要一次中断，收到之后就可以通过轮循的方式，把队列内存中所有的数据都拿走，达到非常高效的状态。

==== 驱动监控

[source,bash]
.ethtool -S
----
andrew@andrew-G3-3590:~$ ethtool -S wlp4s0
NIC statistics:
     rx_packets: 2778911
     rx_bytes: 2758807450
     rx_duplicates: 5
     rx_fragments: 2468225
     rx_dropped: 1185
     tx_packets: 927109
     tx_bytes: 101761735
     tx_filtered: 0
     tx_retry_failed: 0
     tx_retries: 0
     ...
----

使用ethtool工具可以拿到网卡中常规的统计信息，而这里最值得关注的当然是那些异常的信息.

同样可以通过查看系统的net信息获取这些信息，`/sys/class/net/<interface>/statistics/` 目录中会记录对应网卡的一些信息统计。

[source,back]
----
andrew@andrew-G3-3590:~$ cat /sys/class/net/wlp4s0/statistics/rx_
rx_bytes          rx_dropped        rx_frame_errors   rx_nohandler
rx_compressed     rx_errors         rx_length_errors  rx_over_errors
rx_crc_errors     rx_fifo_errors    rx_missed_errors  rx_packets
andrew@andrew-G3-3590:~$ cat /sys/class/net/wlp4s0/statistics/rx_dropped
0
----

相同的信息也会在 `/proc/net/dev` 中进行体现，这些信息差不多，只是获取的方法不一致而已

[source,bash]
----
:~$ cat /proc/net/dev
Inter-|   Receive                                                |  Transmit
 face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed
    lo: 67220678  409483    0    0    0     0          0         0 67220678  409483    0    0    0     0       0          0
enp3s0:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0
enx00e04c68053d:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0
wlp4s0: 2628562869 2161689    0    0    0     0          0         0 102174407  929847    0    0    0     0       0          0
docker0:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0

----


[TIP]
====
1. 如果overruns错误，说明Rx Tx的队列太小了，需要扩大
2. 如果frame错误，一般是网络数据的RAR校验不通过，需要考虑，互联网上的需要考虑数据是否被篡改，网线直连的需要考虑是否网线或者网卡出现了问题
====

==== 调优

调优能够更改的地方比较少，主要针对多队列的调整，比较直观，调整队列数目、大小、各个队列之间的权重、哈希字段等

使用 `ethtool -l <interface>` 可以查看网卡的预设信息

举个例子，比如有个 web server 绑定到了 CPU2，而机器有多个 CPU，这个机器的网卡也是多队列的，其中某个队列会被 CPU2 处理。这个时候就会有一个问题，因为网卡有多个队列，所以 80 端口的流量只会被分配到其中一个队列上去。假如这个队列不是由 CPU2 处理的，就会涉及到一些数据的腾挪。底层把数据接收上来后再交给应用层的时候，需要把这个数据移动一下。如果本来在 CPU1 处理的，需要挪到 CPU2 去，这时会涉及到 CPU cache 的失效，这对高速运转的 CPU 来说是代价很高的操作

我们可以通过前面提到的工具，特意把 80 端口 tcp 数据流量导向到对应 CPU2 处理的网卡队列。这么做的效果是数据包从到达网卡开始，到内核处理完再到送达应用层，都是同一个 CPU。这样最大的好处就是缓存，CPU 的 cache 始终是热的，如此整体下来，它的延迟、效果也会非常好。


==== 内核网络子系统

内核网络子系统，需要关注的主要有软中断于网络子系统

image::../image/image-2022-06-26-18-23-16-598.png[内核网络子系统]

硬件和内核之间交互必须通过中断，这种机制处理其它事情好可以，但是作为网络数据来说如果一个数据就上报一个中断的话会带来如下问题：

1. 中断处理期间，会对之前的中断信信号进行屏蔽，当一个中断处理的时间很长，在处理期间收到的中断信号都会丢掉
2. 一个数据包触发一次中断，当有大量数据包到来后，就会产生大量的中断。网络数据量大时会导致CPU忙于处理中断而没办法干其他事情。

针对以上问题的解决办法就是让终端处理尽量的短。具体来说就是不能在中断中处理的数据，将需要处理的数据丢出来交给软中断进行处理

[source, cpp]
.ksoftirqd
----
// 软中断
static struct smp_hotplug_thread softirq_threads = {
	.store			= &ksoftirqd,
	.thread_should_run	= ksoftirqd_should_run,
	.thread_fn		= run_ksoftirqd,
	.thread_comm		= "ksoftirqd/%u",
};
// 为每一个CPU都注册一个软中断线程，服务器上每一个CPU都会分配一个这样的线程
// 软中断和网络相关的就是 NET-Tx NET-Rx
static __init int spawn_ksoftirqd(void)
{
	register_cpu_notifier(&cpu_nfb);

	BUG_ON(smpboot_register_percpu_thread(&softirq_threads));

	return 0;
}
----

查看软中断信息

[source, bash]
----
# cat /proc/softirqs
                    CPU0       CPU1       CPU2       CPU3
          HI:          0          0          0          0
       TIMER: 2623983768 2668790241 2685144852 2721179533
      NET_TX:       5811       5942       5124    3113782
      NET_RX: 2016198897 1627027198 1593502916 2264407343
       BLOCK:   15697043    9637765   18050603   38241598
BLOCK_IOPOLL:          0          0          0          0
     TASKLET:   15807487   19225321    8343774   16115362
       SCHED: 1192747035 1125818886 1070286566 1060203270
     HRTIMER:          0          0          0          0
         RCU:   86329660   73790543   76646829   91746559

----

内核初始化流程

- 针对每个CPU，创建一个数据结构
- 注册一个软中断处理函数(NET-RX, NET-Tx)

[plantuml, diagram=softwareirq-index, format=png]
----
@startuml
[*] -> NIC : packets
NIC -> RAM : DMA
RAM : RingBuffer
NIC --> CPU : IRQ is raised
NIC --->Driver : IRQ Cleared
CPU --> Driver : RUns IRQ Handler
Driver ->  [*] : NAPI is started
@enduml
----


1. 网卡收到数据包
2. 把数据通过DMA拷贝到内存里面
3. 产生一个中断，告诉CPU并开始处理中断(将中断信号屏蔽->环形NAPI机制)

[source,cpp]
----
static irqreturn_t igb_msix_ring(int irq, void *data)
{
	struct igb_q_vector *q_vector = data;

	/* Write the ITR value calculated from the previous interrupt. */
	igb_write_itr(q_vector);

	napi_schedule(&q_vector->napi);

	return IRQ_HANDLED;
}
----

上面的代码是 igb 网卡驱动中断处理函数做的事情。如果省略掉开始的变量声明和后面的返回，这个中断处理函数只有两行代码，非常短。需要关注的是第二个，在硬件中断处理函数中，只用激活外部 NIPA 软中断处理机制，无需做其他任何事情。因此这个中断处理函数会返回的非常快。

[source, cpp]
----
/* Called with irq disabled */
static inline void ____napi_schedule(struct softnet_data *sd,
				     struct napi_struct *napi)
{
	list_add_tail(&napi->poll_list, &sd->poll_list);
	__raise_softirq_irqoff(NET_RX_SOFTIRQ);
}
----
NIPI 的激活也很简单，主要为两步。内核网络系统在初始化的时每个 CPU 都会有一个结构体，它会把队列对应的信息插入到结构体的链表里。换句话说，每个网卡队列在收到数据的时候，需要把自己的队列信息告诉对应的 CPU，将这两个信息绑定起来，保证某个 CPU 处理某个队列。

除此之外，还要与触发硬中断一样，需要触发软中断。下图将很多步骤放到了一块，前面讲过的就不再赘述了。图中要关注的是软中断是怎么触发的。与硬中断差不多，软中断也有中断的向量表。每个中断号，都会对应一个处理函数，当需要处理某个中断，只需要在对应的中断向量表里找就好了，跟硬中断的处理是一模一样的。

image::../image/image-2022-06-26-23-24-36-402.png[sirq-processing]

*数据接收监控*

每个设备都有独立的中断号，在当前机器上56-59就是王康ens160的收发中断号，正常保证每个CPU都处理一个中断时效率是最高的，如果不是就需要做些调整将这些中断进行分散。

[source, bash]
----
[root@localhost lib]# cat /proc/interrupts
           CPU0       CPU1       CPU2       CPU3
  0:         52          0          0          0   IO-APIC-edge      timer
 56:  386021285  329753204  490753058  368168795   PCI-MSI-edge      ens160-rxtx-0
 57:  215302811  207102966  225180046  902275735   PCI-MSI-edge      ens160-rxtx-1
 58:  942480710  258907890  289008190  258273639   PCI-MSI-edge      ens160-rxtx-2
 59:   90400625  461816820  228991190  378142346   PCI-MSI-edge      ens160-rxtx-3
 60:          0          0          0          0   PCI-MSI-edge      ens160-event-4
----

==== 数据接收调优

- 中断可以做的调优有两个，一是中断合并，二是中断亲和性

.自适应中断合并
****
rx-usecs:数据帧到达后延迟多长时间产生中断信号，单位微秒
rx-frames:触发中断前积累数据帧的最大个数
rx-usecs-irq:如果中断处理正在执行，当前中断延迟多久送达CPU
rx-frames-irq:如果中断正在执行，最多积累多少数据帧
****

上面都是硬件网卡支持的功能，NAPI本质上也是中断合并机制，如果有很多包进来，NAPI就可以做到只产生一个中断，因此不需要使用硬件帮助做中断合并，实际效果和NAPI是相同的，都是用来减少中断的数量。

.中断亲和性
****
`echo 1 > /proc/irq/8/smp_affinity` +
通过设置亲和性，将对应的中断交给指定的CPU处理
****

==== 内核-数据处理

最后是数据处理部分了。当数据到达网卡，进入队列内存后，就需要内核从队列内存中将数据拉出来。如果机器的 PPS 达到了十万甚至百万，而 CPU 只处理网络数据的话，那其他基本的业务逻辑也就不用干了，因此不能让数据包的处理独占整个 CPU，而核心点是怎么去做限制。

整体限制就是一个CPU对应一个队列，如果CPU的数量比队列数量少，那么一个CPU可能需要处理多个队列

单次限制则是一个队列在一轮处理包的数量，达到限制之后就停下来，等待下一轮处理(分而治之的思想)

而停下来是很关键的节点，幸运的是有对应的指标记录，有Time-squeeze这样的中断计数，拿到这个信息就可以判断出机器网络处理是否有瓶颈，被迫中断频次的高低

[source,bash]
----
[root@localhost lib]# cat /proc/net/softnet_stat
5dd1bfb3 00000000 0000002f 00000000 00000000 00000000 00000000 00000000 00000000 00000000
3f009d76 00000000 00000047 00000000 00000000 00000000 00000000 00000000 00000000 00000000
3ccda8dd 00000000 00000030 00000000 00000000 00000000 00000000 00000000 00000000 00000000
3fd62407 00000000 00000035 00000000 00000000 00000000 00000000 00000000 00000000 00000000
----

这些是CPU的指标数据，格式很简单，每一行对应一个CP(顺序从上到下)，输出格式是按照16进制输出，但是没有对应的文档说列对应的信息，需要自己查看对应的内核源码实现

[source, cpp]
----
/*
 * Incoming packets are placed on per-cpu queues
 */
struct softnet_data {
	struct Qdisc		*output_queue;
	struct Qdisc		**output_queue_tailp;
	struct list_head	poll_list;
	struct sk_buff		*completion_queue;
	struct sk_buff_head	process_queue;

	/* stats */
	unsigned int		processed; // 处理的包数量（多网卡 bond 模式可能多于实际的收包数量）
	unsigned int		time_squeeze; // 软中断处理 net_rx_action 被迫打断的次数 说明数据量大CPU忙才会被打断
	unsigned int		cpu_collision; // 发送数据时获取设备锁冲突，比如多个 CPU 同时发送数据
	unsigned int		received_rps; //  当前 CPU 被唤醒的次数（通过处理器间中断）

#ifdef CONFIG_RPS
	struct softnet_data	*rps_ipi_list;

	/* Elements below can be accessed between CPUs for RPS */
	struct call_single_data	csd ____cacheline_aligned_in_smp;
	struct softnet_data	*rps_ipi_next;
	unsigned int		cpu;
	unsigned int		input_queue_head;
	unsigned int		input_queue_tail;
#endif
	unsigned int		dropped;  // 丢包数量，因为队列满了
	struct sk_buff_head	input_pkt_queue;
	struct napi_struct	backlog;
};

// time_squeeze 重点关注time_squeeze
static int softnet_seq_show(struct seq_file *seq, void *v)
{
	struct softnet_data *sd = v;

	seq_printf(seq, "%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n",
		   sd->processed, sd->dropped, sd->time_squeeze, 0,
		   0, 0, 0, 0, /* was fastroute */
		   sd->cpu_collision, sd->received_rps);
	return 0;
}
----




https://netdevconf.org[Linux子网系统历年大会]




https://baijiahao.baidu.com/s?id=1712167199181924621&wfr=spider&for=pc


https://blog.51cto.com/u_15352893/3741281

https://baijiahao.baidu.com/s?id=1712167031976582525&wfr=spider&for=pc

https://baijiahao.baidu.com/s?id=1712167199181924621&wfr=spider&for=pc



