
:toc:

:icons: font

// 保证所有的目录层级都可以正常显示图片
:path: 网络/
:imagesdir: ../image/
:srcdir: ../src


// 只有book调用的时候才会走到这里
ifdef::rootpath[]
:imagesdir: {rootpath}{path}{imagesdir}
:srcdir: {rootpath}../src/
endif::rootpath[]

ifndef::rootpath[]
:rootpath: ../
:srcdir: {rootpath}{path}../src/
endif::rootpath[]


== 网络常见知识点


=== 基础知识

==== bit\frame\packet\segment\data区别

1. 物理层发出的数据成为bit,物理层传输的是比特流。
2. 数据链路层发出的数据包称为frame，地址是链路层的地址，如mac地址；
3. 网络层发出的数据包称为packet，地址是网络层地址，如ip地址；
4. 传输层发出的数据包称为segment，地址是传输层地址，如TCP的端口号。

数据的封装过程：

1. data
2. segment
3. packet
4. frame
5. bit

对应的协议：
1.application-session （Represen layer is in the middle) layer
2.transprot layer
3.network layer
4.data link layer
5.physical layer

'''

datagram（数据报）与packet（数据包）区别：

datagram（数据报） – 在网络层中的传输单元（例如IP）。一个datagram可能被压缩成一个或几个packets，在数据链路层中传输。

packet（数据包） – 封装的基本单元，它穿越网络层和数据链路层的分解面。通常一个packet映射成一个frame，但也有例外：即当数据链路层执行拆分或将几个packet合成一个frame的时候。

二层的PDU叫做Frame;
IP的叫做Packet;
TCP的叫做Segment；
UDP的叫做Datagram。

OSI参考模型的各层传输的数据和控制信息具有多种格式,常用的信息格式包括帧、数据包、数据报、段、消息、元素和数据单元。信息交换发生在对等OSI层之间，在源端机中每一层把控制信息附加到数据中，而目的机器的每一层则对接收到的信息进行分析，并从数据中移去控制信息，下面是各信息单元的说明：

- 帧（frame）是一种信息单位，它的起始点和目的点都是数据链路层。
- 数据包（packet）也是一种信息单位，它的起始和目的地是网络层。
- 数据报（datagram）通常是指起始点和目的地都使用无连接网络服务的的网络层的信息单元。
- 段（segment）通常是指起始点和目的地都是传输层的信息单元。
- 消息（message）是指起始点和目的地都在网络层以上（经常在应用层）的信息单元。
- 元素（cell）是一种固定长度的信息，它的起始点和目的地都是数据链路层。元素通常用于异步传输模式（ATM）和交换多兆位数
- 服务（SMDS）网络等交换环境。
- 数据单元（data unit）指许多信息单元。常用的数据单元有服务数据单元（SDU）、协议数据单元（PDU）。SDU是在同一机器上的两层之间传送信息。PDU是发送机器上每层的信息发送到接收机器上的相应层（同等层间交流用的）。



=== 如果让你设计一个聊天应用层协议，需要考虑哪些问题？
1. 要考虑基于哪些网络层，一般要基于传输层和应用层来分工完成整个聊天应用的通信
2. 应用层还需要传输一些信息来控制程序的运行过程，因此还需要自行定义应用层协议
 - TCP/Websocket协议维护长连接
 - UDP进行消息/文件的传输和接收
 - HTTP/HTTPS完成用户登录，用户注销等
 - 设计心跳保活命令

=== 服务端挂了，客户端的tcp连接还在吗？

- 如果服务端进程崩溃，那么内核会发送FIN报文，与客户端进行四次挥手
tcp的连接是内核维护的，所以当服务端的进程崩溃之后，内核需要回收进程的所有TCP连接资源，因此内核会在回收资源时断开对应的tcp连接，整个过程并不需要对应进程的参与，所以即使服务端进程退出了还是能与客户端完成tcp的四次挥手

- 如果服务端宕机，服务端不会进行四次挥手，后续发生的事情会根据客户端是否重新发送数据变现的不同
1. 如果客户端会发送数据，由于服务端已经不存在了，客户端的数据会一直超时重传，当重传次数超过一定阈值后会断开TCP连接
2. 如果客户端一直不发送数据，需要根据客户端是否开启TCP keepalive机制，如果开启超时检测服务端已经不存在就会断开连接，如果没有开启客户端的tcp会一直存在，并不会断开

服务端发生宕机之后无法和客户端进行四次挥手，所以在服务端发生宕机的时刻起，客户端是无法感知到服务端的宕机的，只能在后续的数据交互来判断服务端是否存在

如果服务端宕机之后客户端会发送数据，在超时之后客户端会启用超时重传机制，用来重传那些发送了但是没有得到回复的数据。当重传次数到达一定阈值之后内核就会判定该TCP连接出现了问题，然后通过socket接口通知该应用该tcp连接出现了问题，客户端在接收到这个问题反馈之后，会断开TCP的连接

Linux中可以通过 tcp_retries2的配置项来设置重传超时时间。
[source, bash]
.tcp_retries2
----
$cat /proc/sys/net/ipv4/tcp_retries2
15
----
tcp_retries2不是代表重传的次数，该数值只是用来计算重传超时的，设置为15大概超时时间为timeout= 924600ms，如果重传间隔超过这个timeout就会认为停止重传，然后就会断开tcp连接

在发生超时重传的过程中，每一轮的超时时间(RTO)都是倍数增长的，如果第一轮的超时时间为200ms那么第二轮的RTO就会调整为400ms......, RTO是根据RTT计算出来的，如果发生超时重传，那么重复几次之后就会达到timeout的值了。

RTO在Linux中使用宏定义进行预定义，以Linux2.6+为例，HZ为1000ms那么TCP_RTO_MIN为200ms， TCP_RTO_MAX为120s

[source, c]
.RTO 定义
----
#define TCP_RTO_MAX ((unsigned)(120*HZ))
#define TCP_RTO_MIN ((unsigned)(HZ/5))
----

=== 位域和大小端

因为C/C++定义中有一个规定，无论是大端还是小端，先定义的成员一定是低字节和低位

1. 大端和小端存储相反，字节顺序相反，高低位相反
2. 那么当位域中先定义的成员一定是低字节和低位的条件限制时，就会出现一个问题，那就是low无论在小端还是大端机器中都是去低字节和低位(4位)，那么大小端中low和high就会存在互换的情况

[source, cpp]
----
struct Data {
    union {
        uint8_t res;
        struct {
           uint8_t low : 4;
           uint8_t high : 4;
        };
    };
};
----




=== 为什么本地通信无论写的什么IP地址最终都会走lo虚拟地址

在内核初始化local路由表的时候，回把路由表项都设置为RTN_LOCAL，这个过程是在设置本机IP的时候调用fib_inetaddr_event实现的，也就是说本机IP的类型都是RTN_LOCAL

[source, cpp]
----
void fib_add_ifaddr(struct in_ifaddr *ifa)
{
	fib_magic(RTM_NEWROUTE, RTN_LOCAL, addr, 32, prim);
}
----

使用本机IP即使不使用127.0.0.1，内核在查找的时候判断类型依然是RTN_LOCAL，也就是net->loopback_dev，也就是本机lo虚拟网卡。

在跨机网络包的接收过程中，需要经过硬中断然后才能出发软中断，在本机网络IO过程中，由于并不是真的过网卡，所以网卡的发送过程、硬中断都省去了直接从软中断开始



=== 网络相关参考

https://coolshell.cn/articles/11564.html


=== 网络问题排查


==== ethtool工具

*查看和修改网卡RingBuffer最大值和当前设置*
[source, bash]
----
# 查看
$ ethtool -g ens160
# 修改网卡硬件缓存大小
$ ethtool -G eth0 rx 4096 tx 4096
----

*查看网卡丢包统计*

[source, bash]
----
$ ethtool -S eth0
----

*查看网卡配置状态*
[source, bash]
----
$ethtool ens160
Settings for ens160:
	Supported ports: [ TP ]
	Supported link modes:   1000baseT/Full
	                        10000baseT/Full
	Supported pause frame use: No
	Supports auto-negotiation: No
	Supported FEC modes: Not reported
	Advertised link modes:  Not reported
	Advertised pause frame use: No
	Advertised auto-negotiation: No
	Advertised FEC modes: Not reported
	Speed: 10000Mb/s
	Duplex: Full
	Port: Twisted Pair
	PHYAD: 0
	Transceiver: internal
	Auto-negotiation: off
	MDI-X: Unknown
Cannot get wake-on-lan settings: Operation not permitted
	Link detected: yes
----

如果通过ethtool查看网卡的speed等配置不符合预期，可以通过ethtool让网卡自动协商。

[source, bash]
----
$ethtool -r  eth1/eth0;
----

如果发现，上下游设备不支持自协商，可以强制设置端口的速率：
[source, bash]
----
$ethtool -s eth1 speed 1000 duplex full autoneg off
----

*查看流控统计信息*

[source, bash]
----
$ethtool -S eth1 | grep control
----

*查看网络流控配置*
[source, bash]
----
$ethtool -a eth1
----

*关闭网卡流控*
[source, bash]
----
ethtool -A ethx autoneg off //自协商关闭
ethtool -A ethx tx off //发送模块关闭
ethtool -A ethx rx off //接收模块关闭
----

*查看网卡firmware版本*
[source, bash]
----
$ethtool -i eth0
----

*单核负载过高导致丢包*
当单核心因为软中断占用过多时会导致新来的数据软中断无法处理而丢包，导致网络数据的处理速度跟不上网卡的接收速度。
通过命令mpstat -P ALL 1可以每隔一秒查看一下当前cpu的使用情况
[source, bash]
----
mpstat -P ALL 1
Linux 3.10.0-862.11.6.el7.x86_64 (localhost.localdomain) 	10/26/2023 	_x86_64_	(4 CPU)
01:27:31 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
01:27:32 PM  all    7.30    0.00    7.03    0.00    0.00    0.00    0.00    0.00    0.00   85.68
01:27:32 PM    0    5.43    0.00    7.61    0.00    0.00    0.00    0.00    0.00    0.00   86.96
01:27:32 PM    1    7.45    0.00    7.45    0.00    0.00    0.00    0.00    0.00    0.00   85.11
01:27:32 PM    2    5.43    0.00    5.43    0.00    0.00    1.09    0.00    0.00    0.00   88.04
01:27:32 PM    3   11.83    0.00    6.45    0.00    0.00    0.00    0.00    0.00    0.00   81.72
----

如果出现这种情况，比如soft占用100%，这个时候可以通过一下几种方法中的一种或者多种组合来解决问题：

- 调整RSSfootnote:[Receive Side Scaling,用于多核系统中分发网络流量到不同的CPU核心进行处理]队列。

[source, bash]
----
# 查看RSS队列配置
ethtool -x eth0
# 调整配置
ethtool -X eth0 xxx
----

- 保证网卡中断配置均衡 `cat /proc/interrupts`

[source, bash]
----
# 调整irq
irqbalance
# 查看当前运行情况
service irqbalance status
# 终止服务
service irqbalance stop
#中断绑CPU核
echo mask > /proc/irq/xxx/smp_affinity
----

- 根据CPU和网卡队列个数调整网卡多队列和RPS配置

协议栈开启RPS并设置RPS
[source, bash]
----

echo $mask（CPU配置）> /sys/class/net/$eth/queues/rx-$i/rps_cpusecho 4096（网卡buff）> /sys/class/net/$eth/queues/rx-$i/rps_flow_cnt2）
#CPU小于网卡队列个数，绑中断就可以，可以试着关闭RPS看一下效果：
echo 0 > /sys/class/net/<dev>/queues/rx-<n>/rps_cpu
----

- numa CPU调整，对其网卡位置，可以提高内核处理速度，从而给更多CPU应用包，从而减缓丢包率
[source, bash]
----
ethtool -i eth1|grep bus-info
lspci -s bus-info -vv|grep node
----
上面中断和 RPS 设置里面 mask 需要重新按 numa CPU 分配重新设置；

- 可以试着开启中断聚合(需要网卡支持)

[source, bash]
----
# 查看是否支持
ethtool -c ethx
# 调整
ethtool -C ethx adaptive-rx on
----

*分片重组导致丢包*

[source, bash]
----
# 查看是否有分片重组导致丢包发生
netstat -s|grep timeout
601 fragments dropped after timeout
# 通过调整分片重组超时时间来解决
# 通过
sysctl -a| grep net.ipv4.ipfrag_time
net.ipv4.ipfrag_time = 30
sysctl -w net.ipv4.ipfrag_time=60
----



=== 连接失败问题查询

主要通过观察listen队列和accept队列的问题来断定连接创建失败时由于具体哪个地方导致的问题。

.通过ss查看accept队列的大小以及当前队列中现存待连接完成的个数
[source, bash]
----
# -l 显示正在监听的socket
# -n 不解析服务器名称
# -t 只显示tcp socket
[wangyz@localhost /home/wangyz]$ss  -lnt
State       Recv-Q Send-Q                             Local Address:Port                                            Peer Address:Port
LISTEN      0      100                                    127.0.0.1:25                                                         *:*
LISTEN      0      128                                    127.0.0.1:6010                                                       *:*
LISTEN      0      100                                            *:16506                                                      *:*
LISTEN      0      100                                            *:20571                                                      *:*
LISTEN      0      50                                             *:8091                                                       *:*
LISTEN      0      128                                  10.20.26.58:36379                                                      *:*
----

Recv-Q代表待accept的个数，也就是队列中有多少连接，Send-Q代表队列的最大值，队列的大小长度取决于somaxconn配置和backlog之间的最小值，min(somaxconn, backlog)，somaxconn是内核配置，backlog是listen中传入

[source, bash]
----
[wangyz@localhost /home/wangyz]$date;netstat -s|grep overflowed
2024年 06月 05日 星期三 14:22:15 CST
    48883 times the listen queue of a socket overflowed
----

通过netstat查看listen溢出的数量，如果一直在增加说明存在accept溢出问题。

