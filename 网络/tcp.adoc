
:toc:

:icons: font

// 保证所有的目录层级都可以正常显示图片
:path: 网络/
:imagesdir: ../image/
:srcdir: ../src


// 只有book调用的时候才会走到这里
ifdef::rootpath[]
:imagesdir: {rootpath}{path}{imagesdir}
:srcdir: {rootpath}../src/
endif::rootpath[]

ifndef::rootpath[]
:rootpath: ../
:srcdir: {rootpath}{path}../src/
endif::rootpath[]

== tcp


=== tcp哪些不得不知的事情

.TCP
****
TCP很复杂，复杂到难以想象，很多人抱怨为什么TCP不能精简点，有这样想法的人是没有看TCP的发展史。因为实际网络中的各种不确定情况都需要靠TCP来解决，而这些问题还会导致一些列子问题，问题带问题子子孙孙无穷尽已。但是这并不是说TCP不值得学，而是TCP太值得学习了，你通过学习TCP能学习很多问题的解决方案，也能学习到很多best practice，而这些都是经过无数次实践检验的宝贵经验。
****

- TCP的头格式包括源端口和目标端口，序号，确认号，窗口大小和包类型等关键信息。
- TCP的状态机用于维护连接状态，建立和断开连接需要不同数量的握手和挥手。
- TCP使用序号来解决包乱序问题，使用确认号来解决丢包问题。
- TCP的重传机制包括超时重传和快速重传，用于确保数据的可靠传输。
- TCP中的超时重传机制可以选择只重传超时的包或者重传超时后的所有包。
- TCP中的快速重传机制可以在接收到乱序的包后立即重传丢失的包。
- TCP中的重传机制会带来带宽和效率的损失，因此不是一个理想的解决方案。
- TCP使用时间等待状态来保证连接的正常关闭，在连接关闭后要等待一段时间才能释放资源。
- TCP的时间等待状态可以通过调整参数来控制并发连接的数量。

- 为什么要记录那么多，因为TCP真他妈太复杂了，不记录一些知识点肯定会被忘记的
- 经过30多年的修改，TCP的复杂程度已经到了无以复加的程度，所以很多组织和公司开始绕过内核的tcp来实现一些功能如：DPDK, XDP

==== tcp头格式

image::image-2023-06-12-11-21-27-954.png[ipheader]
image::image-2023-06-12-11-21-47-333.png[tcpheader]
image::image-2023-06-12-11-25-21-439.png[tcpoptions]
image::image-2023-06-12-11-22-04-655.png[udpheader]

这里我们只看tcp的：

- TCP的包是没有IP地址的，那是IP层上的事。但是有源端口和目标端口。
- 一个TCP连接需要四个元组来表示是同一个连接（src_ip, src_port, dst_ip, dst_port）准确说是五元组，还有一个是协议。但因为这里只是说TCP协议，所以，这里我只说四元组。
- 注意上图中的四个非常重要的东西：
    1. Sequence Number是包的序号，用来解决网络包乱序（reordering）问题。
    2. Acknowledgement Number就是ACK——用于确认收到，用来解决不丢包的问题。
    3. Window又叫Advertised-Window，也就是著名的滑动窗口（Sliding Window），用于解决流控的。
    4. TCP Flag ，也就是包的类型，主要是用于操控TCP的状态机的。

==== tcp的状态机

IP是没有连接的，为什么建立在IP协议之上的TCP就是有连接的？这是因为TCP所谓的连接，其实质不过是通讯双方维护的一个连接状态，让它看起来好像有连接一样，因此、了解tcp状态变换对于理解tcp非常重要。

image::image-2023-06-12-11-24-03-990.png[tcp状态机]
image::image-2023-06-12-13-28-46-574.png[tcp timeline]

建立连接为什么要3次握手，断开连接为什么要4次挥手？

- 对于建立连接的三次握手：主要来初始化序列号(sequence number)，通信双方需要通知对方自己的sequence number初始值(ISN: Initial sequence number) -- 也叫SYN,全称synchronize sequence number。初始这些号就是为了解决网络传输出现乱序的问题，出现乱序之后tcp使用这些序号对数据进行拼接。

- 4次挥手：如果有一方发送了fin报文，对方需要回复对方ack，并向复方发送ack。如果双方都有连接管理，也可能同时发送fin给对方。在完成四次挥手之后客户端和服务端都会进入time_wait状态

[caption="tcp四次挥手: ", link=http://www.tcpipguide.com/free/t_TCPConnectionTermination-4.htm]
image::image-2023-06-12-13-30-37-335.png[]
http://www.tcpipguide.com/free/t_TCPConnectionTermination-4.htm[图片来源]

- 建立连接时SYN超时：试想一下， 如果server端接到了clien发的SYN后回了SYN-ACK后client掉线了，server端没有收到client回来的ACK，那么这个时候server的连接处于一个中间状态，即没成功，也没失败。server端如果在一定时间内没有收到会重发SYN-ACK。在Linux下，默认重试次数为5次，第 1 次重试发生在 1 秒钟后，接着会以翻倍的方式在第 2、4、8、16、32 秒共做 6 次重试，最后一次重试会等待 64 秒，如果仍然没有返回 ACK，才会终止三次握手。所以，总耗时是 1+2+4+8+16+32+64=127 秒，超过 2 分钟，TCP才会把断开这个连接。
- 关于SYN Flood攻击。一些恶意的人就为此制造了SYN Flood攻击——给服务器发了一个SYN后，就下线了，于是服务器需要默认等63s才会断开连接，这样，攻击者就可以把服务器的syn连接的队列耗尽，让正常的连接请求不能处理。于是，Linux下给了一个叫tcp_syncookies的参数来应对这个事——当SYN队列满了后，TCP会通过源地址端口、目标地址端口和时间戳打造出一个特别的Sequence Number发回去（又叫cookie），如果是攻击者则不会有响应，如果是正常连接，则会把这个 SYN Cookie发回来，然后服务端可以通过cookie建连接（即使你不在SYN队列中）。请注意， [red]*请先千万别用tcp_syncookies来处理正常的大负载的连接的情况* 。因为，synccookies是妥协版的TCP协议，并不严谨。对于正常的请求，你应该调整三个TCP参数可供你选择，第一个是：tcp_synack_retries 可以用他来减少重试次数；第二个是：tcp_max_syn_backlog，可以增大SYN连接数；第三个是：tcp_abort_on_overflow 处理不过来干脆就直接拒绝连接了。

image::../image/image-2023-06-13-11-10-11-967.png[]

- 关于ISN的初始化。ISN是不能hard code的，不然会出问题的——比如：如果连接建好后始终用1来做ISN，如果client发了30个segment过去，但是网络断了，于是 client重连，又用了1做ISN，但是之前连接的那些包到了，于是就被当成了新连接的包，此时，client的Sequence Number 可能是3，而Server端认为client端的这个号是30了。全乱了。RFC793中说，ISN会和一个假的时钟绑在一起，这个时钟会在每4微秒对ISN做加一操作，直到超过2^32，又从0开始。这样，一个ISN的周期大约是4.55个小时。因为，我们假设我们的TCP Segment在网络上的存活时间不会超过Maximum Segment Lifetime（缩写为MSL – Wikipedia语条），所以，只要MSL的值小于4.55小时，那么，我们就不会重用到ISN。
- 关于 MSL 和 TIME_WAIT。通过上面的ISN的描述，相信你也知道MSL是怎么来的了。我们注意到，在TCP的状态图中，从TIME_WAIT状态到CLOSED状态，有一个超时设置，这个超时设置是 2*MSL（RFC793定义了MSL为2分钟，Linux设置成了30s）为什么要这有TIME_WAIT？为什么不直接给转成CLOSED状态呢？主要有两个原因：1）TIME_WAIT确保有足够的时间让对端收到了ACK，如果被动关闭的那方没有收到Ack，就会触发被动端重发Fin，一来一去正好2个MSL，2）有足够的时间让这个连接不会跟后面的连接混在一起（你要知道，有些自做主张的路由器会缓存IP数据包，如果连接被重用了，那么这些延迟收到的包就有可能会跟新连接混在一起）。你可以看看这篇文章《 https://serverframework.com/asynchronousevents/2011/01/time-wait-and-its-design-implications-for-protocols-and-scalable-servers.html[TIME_WAIT and its design implications for protocols and scalable client server systems] 》
- 关于TIME_WAIT数量太多。从上面的描述我们可以知道，TIME_WAIT是个很重要的状态，但是如果在大并发的短链接下，TIME_WAIT 就会太多，这也会消耗很多系统资源。只要搜一下，你就会发现，十有八九的处理方式都是教你设置两个参数，一个叫tcp_tw_reuse，另一个叫tcp_tw_recycle的参数，这两个参数默认值都是被关闭的，后者recyle比前者resue更为激进，resue要温柔一些。另外，如果使用tcp_tw_reuse，必需设置tcp_timestamps=1，否则无效。这里，你一定要注意，打开这两个参数会有比较大的坑——可能会让TCP连接出一些诡异的问题（因为如上述一样，如果不等待超时重用连接的话，新的连接可能会建不上。正如官方文档上说的一样“It should not be changed without advice/request of technical experts”）。
    a. 关于tcp_tw_reuse。官方文档上说tcp_tw_reuse 加上tcp_timestamps（又叫PAWS, for Protection Against Wrapped Sequence Numbers）可以保证协议的角度上的安全，但是你需要tcp_timestamps在两边都被打开（你可以读一下tcp_twsk_unique的源码 ）。我个人估计还是有一些场景会有问题。
    b. 关于tcp_tw_recycle。如果是tcp_tw_recycle被打开了话，会假设对端开启了tcp_timestamps，然后会去比较时间戳，如果时间戳变大了，就可以重用。但是，如果对端是一个NAT网络的话（如：一个公司只用一个IP出公网）或是对端的IP被另一台重用了，这个事就复杂了。建链接的SYN可能就被直接丢掉了（你可能会看到connection time out的错误）（如果你想观摩一下Linux的内核代码，请参看源码 tcp_timewait_state_process）。
    c. 关于tcp_max_tw_buckets。这个是控制并发的TIME_WAIT的数量，默认值是180000，如果超限，那么，系统会把多的给destory掉，然后在日志里打一个警告（如：time wait bucket table overflow），官网文档说这个参数是用来对抗DDoS攻击的。也说的默认值180000并不小。这个还是需要根据实际情况考虑。

Again，使用tcp_tw_reuse和tcp_tw_recycle来解决TIME_WAIT的问题是非常非常危险的，因为这两个参数违反了TCP协议（RFC 1122）

其实，TIME_WAIT表示的是你主动断连接，所以，这就是所谓的“不作死不会死”。试想，如果让对端断连接，那么这个破问题就是对方的了，呵呵。另外，如果你的服务器是于HTTP服务器，那么设置一个HTTP的KeepAlive有多重要（浏览器会重用一个TCP连接来处理多个HTTP请求），然后让客户端去断链接（你要小心，浏览器可能会非常贪婪，他们不到万不得已不会主动断连接）。

==== 数据传输中的Sequence Number

在wireshark中观察seqnum三次握手总是从0开始，这个是不是和rfc中说的seqnum总是递增的说法违背？其实这时wireshark的一个优化，为了使seqnum显示更加人性化，wireshark中使用了相对序号 relative sqenum，如果想要查看真实的seqnum可以通过右键之后取消relative seqnum功能即可


image::image-2023-06-13-16-05-42-056.png[]

==== TCP重传机制

tcp作为可靠通信，必须保证所有数据包都可以到达，所以必须有重传机制。

____
接收端Ack确认时只会恢复最后一个连续的包，比如发送端发送了1,2,3,4,5，接收端如果接收到了1,2于是就会恢复Ack3，然后收到了4(注意此时3没有收到)。这个时候tcp要怎么办？我们都知道Seqnum和Ack是以字节数为单位，所以ack的时候不能跳着确认，只能确认最大的连续收到的包，不然发送端就会以为之前的包都收到了。
____

===== 超时重传机制
如果采用死等3的方式，当发送方发现收不到3的ack超时后，会重传3，一旦接收3成功，会ack回复4说明3和4都收到了

但是，这种方式会有比较严重的问题，那就是因为要死等3，所以会导致4和5即便已经收到了，而发送方也完全不知道发生了什么事，因为没有收到Ack，所以，发送方可能会悲观地认为也丢了，所以有可能也会导致4和5的重传。

对此有两种选择：

- 一种是仅重传timeout的包。也就是第3份数据。
- 另一种是重传timeout后所有的数据，也就是第3，4，5这三份数据。

这两种方式有好也有不好。第一种会节省带宽，但是慢，第二种会快一点，但是会浪费带宽，也可能会有无用功。但总体来说都不好。因为都在等timeout，timeout可能会很长（在下篇会说TCP是怎么动态地计算出timeout的）

===== 快速重传机制
于是，TCP引入了一种叫Fast Retransmit 的算法，不以时间驱动，而以数据驱动重传。也就是说，如果，包没有连续到达，就ack最后那个可能被丢了的包，如果发送方连续收到3次相同的ack，就重传。Fast Retransmit的好处是不用等timeout了再重传。

比如：如果发送方发出了1，2，3，4，5份数据，第一份先到送了，于是就ack回2，结果2因为某些原因没收到，3到达了，于是还是ack回2，后面的4和5都到了，但是还是ack回2，因为2还是没有收到，于是发送端收到了三个ack=2的确认，知道了2还没有到，于是就马上重转2。然后，接收端收到了2，此时因为3，4，5都收到了，于是ack回6。示意图如下：

[caption=" ", link=https://www.researchgate.net/figure/An-illustration-of-TCPs-sliding-window-mechanism-with-a-fixed-window-size_fig4_221353829]
image::image-2023-06-13-17-14-30-519.png[]

但是fast retransmit只解决了一个问题，就是timeout的问题，还有一个问题必须面对，就是重传之前的一个还是把所有的都重传的问题。在发送端来看，并不知道这个ack2对应哪个包，是缺失是#2,#3,#4,#5...哪个导致的ack2发送端并不清楚，发送端很有可能要把所有的都重传一下。可见快速重传机制可能导致无效数据的发送。

===== SACK 方法

另外一个更好的方法叫：selective acknowledgement(SACK)(参考RFC2018)，这种方式是在tcp头里面添加一个叫做sack的东西，ack还是fast retransmit的Ack，SACK则是汇报收到的数据碎片。总之就是让ack还担任自己的工作，告诉发送端我接收的连续数据的最大值，然后增加ACK来告诉发送端，你有这些碎片数据不需要进行重复发送了，我已经收到了。

image::image-2023-06-14-09-56-34-154.png[]

有个SACK之后，发送端就可以根据回传的SACK来确认接收端收到了哪些数据，这样就可以作为fast retransmit算法的补充。当然这个功能需要双方都支持的时候才有用，在linux中可以通过tcp_sack参数打开这个功能。

这里还需要注意一个问题——接收方Reneging，所谓Reneging的意思就是接收方有权把已经报给发送端SACK里的数据给丢了。这样干是不被鼓励的，因为这个事会把问题复杂化了，但是，接收方这么做可能会有些极端情况，比如要把内存给别的更重要的东西。所以，发送方也不能完全依赖SACK，还是要依赖ACK，并维护Time-Out，如果后续的ACK没有增长，那么还是要把SACK的东西重传，另外，接收端这边永远不能把SACK的包标记为Ack。

____
SACK会消费发送方的资源，试想，如果一个攻击者给数据发送方发一堆SACK的选项，这会导致发送方开始要重传甚至遍历已经发出的数据，这会消耗很多发送端的资源。详细的东西请参看《TCP SACK的性能权衡》
____

===== Duplicate SACK – 重复收到数据的问题

Duplicate SACK又称D-SACK，其主要使用了SACK来告诉发送方有哪些数据被重复接收了。RFC-2883 里有详细描述和示例。下面举几个例子（来源于 https://datatracker.ietf.org/doc/html/rfc2883#autoid-5[RFC-2883]）

.RFC2883 autoid-4
____
This section specifies the use of SACK blocks when the SACK option is used in reporting a duplicate segment.  When D-SACK is used, the
first block of the SACK option should be a D-SACK block specifying the sequence numbers for the duplicate segment that triggers the
acknowledgement.
____

D-SACK使用了SACK的第一个段来做标志，

如果SACK的第一个段的范围被ACK所覆盖，那么就是D-SACK
如果SACK的第一个段的范围被SACK的第二个段覆盖，那么就是D-SACK

*示例一：ACK丢包*

下面的示例中，丢了两个ACK，所以，发送端重传了第一个数据包（3000-3499），于是接收端发现重复收到，于是回了一个SACK=3000-3500，因为ACK都到了4000意味着收到了4000之前的所有数据，所以这个SACK就是D-SACK——旨在告诉发送端我收到了重复的数据，而且我们的发送端还知道，数据包没有丢，丢的是ACK包。

[source, cpp]
----
  Transmitted  Received    ACK Sent
  Segment      Segment     (Including SACK Blocks)

  3000-3499    3000-3499   3500 (ACK dropped)
  3500-3999    3500-3999   4000 (ACK dropped)
  3000-3499    3000-3499   4000, SACK=3000-3500
                                        ---------
----

 示例二，网络延误

下面的示例中，网络包（1000-1499）被网络给延误了，导致发送方没有收到ACK，而后面到达的三个包触发了“Fast Retransmit算法”，所以重传，但重传时，被延误的包又到了，所以，回了一个SACK=1000-1500，因为ACK已到了3000，所以，这个SACK是D-SACK——标识收到了重复的包。

这个案例下，发送端知道之前因为“Fast Retransmit算法”触发的重传不是因为发出去的包丢了，也不是因为回应的ACK包丢了，而是因为网络延时了。

[source, cpp]
----
    Transmitted    Received    ACK Sent
    Segment        Segment     (Including SACK Blocks)

    500-999        500-999     1000
    1000-1499      (delayed)
    1500-1999      1500-1999   1000, SACK=1500-2000
    2000-2499      2000-2499   1000, SACK=1500-2500
    2500-2999      2500-2999   1000, SACK=1500-3000
    1000-1499      1000-1499   3000
                   1000-1499   3000, SACK=1000-1500
                                          ---------
----

==== TCP的RTT算法

我们知道在TCP重传机制中timeout的设置对于重传非常重要。

- 如果设置长了，重发过程就慢，数据丢了老半天才开始重发，没有效率性能特别差
- 设短了，会导致可能没有丢就重发。重发快导致网络拥塞，导致更多的超时，更多的超时导致更多的重发。

而且这个超时时间在不同网络环境下需要设置不同的值，并不能写死一个值。只能动态地进行设置。为了动态的进行设置，Tcp引入了RTT-Round trip time, 也就是一个数据包从发送出去到回来的时间，这样发送端就大概知道用多少时间，从而可以方便的设置timeOut-RTO(Restransmission Timeout)，以让我们的重传机制更加高效。那么这个算法就只是简单的记录下从发送到接收一个数据包的时间这么简单吗？很显然不是的如果只是简单的这样计算那么会引入更多的问题，比如这次计算时间时很可能因为网络问题导致计算的时间过长或者过短，那么还是会有开头所说的问题。

===== 经典算法

https://datatracker.ietf.org/doc/rfc793/[RFC793] 中定义的经典算法是这样的：

- 首先对RTT进行采样，记录下最近几次的RTT值
- 然后做平滑计算SRTT(Smoothed RTT)。公式为(其中的 stem:[\alpha] 取值在0.8到0.9之间，这个算法英文叫做Exponential weighted moving average，中文叫做：加权移动平均)

.Exponential weighted moving average
[stem]
++++
\text { EstRTT }=\left(\alpha \star \text { EstRTT }\right)+\left((1-\alpha) \star \text { SampleRTT }\right)
++++

基于EstRTT设置Timeout时间， stem:[TimeOut = 2 \star  \text { EstRTT }]

- 开始计算RTO。公式如下

[stem]
++++
\text { RTO }=\min \left[\text { UBOUND, } \max \left[\text { LBOUND, }\left(\beta \star \text { SRTT) }\right]\right]
++++

其中：

- UBOUND是最大的timeout时间，上限值
- LBOUND是最小的timeout时间，下限值
- β 值一般在1.3到2.0之间。

====== Karn / Partridge 算法

上面算法解决了RTT计算问题，但是在重传时还是会有问题，你是使用第一次发送数据的时间和ack回来的时间来做为RTT样本值，还是用重传回来的时间和ACK回来的时间做RTT样本值？

这个问题你无论选哪个都有对应的问题：

1. 情况a时，如果采用首次发送和重传之后的ack来计算时间，那么明显时间偏大
2. 情况b时，如果采用重传和首次返回的ack计算时间，那么明显时间偏小

.Illustration of the problem
image::image-2023-06-15-09-28-22-198.png[]

https://people.duke.edu/~zg70/courses/ECE356/slides/19TCPReliableTransimission.pdf[图片来源]

也正是因为有这些问题，所以Karn / Partridge 算法应运而生，这个算反英文原文是这样写的：

. Do not sample RTT when retransmitting
. Double timeout after each retransmission

我的理解就是，不要把重传作为RTT的样本，每次发生重传将timeout的时间扩大一倍。

为什么要每次重传都要对timeout进行翻倍？因为如果不翻倍会引发一个bug--如果在某一时间网络闪动，突然变慢了，产生了比较大的延时，这个延时会导致所有的包进行重传(因为之前设置的timeout很小)，因为重传的不进行RTT采样，所以计算出来的timeout时间就不会更新，这样karn算法将会是一个灾难。于是才有了 Double timeout after each retransmission。很显然的是这样简单的进行估算的算法其实也不是很靠谱。

https://corporatefinanceinstitute.com/resources/capital-markets/exponentially-weighted-moving-average-ewma/[EMA]

====== Jacobson / Karels 算法

前面两种算法严格来说都是加权移动平均，这种算法最大的毛病就是如果RTT有一个大的波动会被平滑掉，很难被发现。所以在1988年又有人推出一个算法叫Jacobson / Karels Algorithm(参见： https://datatracker.ietf.org/doc/html/rfc6298[RFC6289])。公式如下：

原文中这样介绍：

• Difference = SampleRTT − EstimatedRTT
• EstimatedRTT = EstimatedRTT + ( !× Difference)
• Deviation = Deviation + ! (|Difference| − Deviation)
• TimeOut = µ × EstimatedRTT + ϕ × Deviation
    - ! is between 0 and 1
    - Based on experience, µ is typically set to 1 and ϕ is set to 4.
Thus, when the variance is small, TimeOut is close to
EstimatedRTT; a large variance causes the deviation term
to dominate the calculation.

[stem]
++++
\text{SRTT}=\text{SRTT}+\alpha(\text{RTT}-\text{SRTT})-- \text { 计算平滑RTT }
++++

[stem]
++++
\text{DevRTT}=(1-\beta)\star \text{DevRTT}+\beta\star(|\text{RTT}-\text{SRTT}|)--\text {计算平滑RTT和真实的差距（加权移动平 均)}
++++

[stem]
++++
\text {RTO} = µ \star \text {SRTT} + ∂ \star \text{DevRTT} ——\text{ 神一样的公式}
++++

（其中：在Linux下，α = 0.125，β = 0.25， μ = 1，∂ = 4 ——这就是算法中的“调得一手好参数”，nobody knows why, it just works…） 最后的这个算法在被用在今天的TCP协议中（Linux的源代码在：tcp_rtt_estimator）。

==== TCP滑动窗口

[.big]##如##果你说你不知道tcp滑动窗口这个事，就间接等于你不知道tcp协议，我们都知道，[blue]#TCP必须要解决可靠传输和乱序问题#。因此、tcp必须要掌握网络实际的数据处理带宽和数据的处理速度，这样才能不引起网络拥塞导致丢包的发生

所以tcp引入了一些技术和设计来做流控，sliding window是其中一种技术。前面我们说过，*TCP头里面有一个字段叫window，又叫做Advertised-Window,这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据，于是发送端就可以根据这个接收端的接收能力(缓冲区大小)，来发送数据，而不会导致接收端处理不过来*。

image::image-2023-06-15-13-49-44-864.png[]

- 接收端LastByteRead指向了TCP缓冲区中读到的位置，NextByteExpected指向的地方是收到的连续包的最后一个位置，LastByteRcved指向的是收到的包的最后一个位置，我们可以看到中间有些数据还没有到达，所以有数据空白区。
- 发送端的LastByteAcked指向了被接收端Ack过的位置（表示成功发送确认），LastByteSent表示发出去了，但还没有收到成功确认的Ack，LastByteWritten指向的是上层应用正在写的地方。

所以：

- 接收端在给发送端回ACK中会汇报自己的AdvertisedWindow = MaxRcvBuffer – LastByteRcvd – 1;
- 而发送方会根据这个窗口来控制发送数据的大小，以保证接收方可以处理。

我们来看下滑动窗口示意图：

image::image-2023-06-15-14-12-42-231.png[]

上图中有四个部分，分别是：

1. ^#1^已经收到ack确认的数据部分
2. ^#2^已经发送出去还没有收到ack确认的部分
3. ^#3^在窗口中还没有进行发送的数据(接收方还有空间，对方还有窗口可以进行发送，能够进行发送)
4. ^#4^窗口以外的数据，按照接收方给的窗口，这部分数据在对方窗口容纳的范围之外

下面是个滑动后的示意图（收到36的ack，并发出了46-51的字节）：

image::image-2023-06-15-14-53-20-285.png[]

下面我们来看一个接受端控制发送端的图示：

image::image-2023-06-15-14-53-41-098.png[]

===== Zero Window

当服务端处理缓慢时，会逐渐将客户端的滑动窗口降至为0，如果window变成0发送端将会停止发送数据。这时有个问题，既然发送端停止发送数据了，那么发送端又如何知道何时能再次开始发送数据呢？

为了让发送端知道服务端有新的窗口可用，tcp引入了zero window probe技术，缩写为ZWP。当发送端窗口变为0时，会发送ZWP包给接收方，接收方接收到之后会使用ack来回复它的window大小，一般会尝试三次，每次大约30-60秒(不同的实现可能不一样)。如果三次之后还是0的话，部分tcp会直接发送RST把连接断开。

____
只要有等待的地方都可能出现DDoS攻击，Zero Window也不例外，一些攻击者会在和HTTP建好链发完GET请求后，就把Window设置为0，然后服务端就只能等待进行ZWP，于是攻击者会并发大量的这样的请求，把服务器端的资源耗尽。
____

===== Silly Window Syndrome

Silly Window Syndrome中文又叫做愚笨窗口综合征，当接收方太忙来不及取走receive window里的数据，就会导致发送方发送的数据越来越小。最后接收方腾出来几个字节并告诉发送方，发送方就会按照几个字节的空位开始发送数据。

如果抛开其他的单独讨论这几个字节是没有问题的，有几个发送几个也没有什么吗。但是要知道这几个字节上面有tcp头，tcp上面还有IP头，ip上面还有mac头，单Tcp+ip头部信息就40个字节了。为了这几个字节要这么大的开销，对于带宽是极大地浪费。

另外，你需要知道网络上有个MTU，对于以太网来说，MTU是1500字节，除去TCP+IP头的40个字节，真正的数据传输可以有1460，这就是所谓的MSS（Max Segment Size）注意，TCP的RFC定义这个MSS的默认值是536，这是因为 RFC 791里说了任何一个IP设备都得最少接收576尺寸的大小（实际上来说576是拨号的网络的MTU，而576减去IP头的20个字节就是536）。

[red]#如果你的网络包可以塞满MTU，那么你可以用满整个带宽，如果不能，那么你就会浪费带宽#。（大于MTU的包有两种结局，一种是直接被丢了，另一种是会被重新分块打包发送） 你可以想像成一个MTU就相当于一个飞机的最多可以装的人，如果这飞机里满载的话，带宽最高，如果一个飞机只运一个人的话，无疑成本增加了，也而相当二。

所以，Silly Windows Syndrome这个现像就像是你本来可以坐200人的飞机里只做了一两个人。 要解决这个问题也不难，就是避免对小的window size做出响应，直到有足够大的window size再响应，这个思路可以同时实现在sender和receiver两端。

- 如果这个问题是由Receiver端引起的，那么就会使用 David D Clark’s 方案。在receiver端，如果收到的数据导致window size小于某个值，可以直接ack(0)回sender，这样就把window给关闭了，也阻止了sender再发数据过来，等到receiver端处理了一些数据后windows size 大于等于了MSS，或者，receiver buffer有一半为空，就可以把window打开让send 发送数据过来。
- 如果这个问题是由Sender端引起的，那么就会使用著名的 Nagle’s algorithm。这个算法的思路也是延时处理，他有两个主要的条件：1）要等到 Window Size>=MSS 或是 Data Size >=MSS，2）收到之前发送数据的ack回包，他才会发数据，否则就是在攒数据。

另外，Nagle算法默认是打开的，所以，对于一些需要小包场景的程序——比如像telnet或ssh这样的交互性比较强的程序，你需要关闭这个算法。你可以在Socket设置TCP_NODELAY选项来关闭这个算法（关闭Nagle算法没有全局参数，需要根据每个应用自己的特点来关闭）

`setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&value,sizeof(int));`

另外，网上有些文章说TCP_CORK的socket option是也关闭Nagle算法，这不对。TCP_CORK其实是更新激进的Nagle算法，完全禁止小包发送，而Nagle算法没有禁止小包发送，只是禁止了大量的小包发送。最好不要两个选项都设置。

==== TCP的拥塞处理 – Congestion Handling

其实通过滑动窗口已经能起到流控(Flow Control)的效果，但是TCP觉得这样还远远不够。因为滑动窗口需要依赖连接的发送端和接收端，其并不知道网络中发生了什么，TCP作为一个那么牛逼的算法，必须能够掌控全局，因此要能感知到整个网络上发生的事情。

具体一点，我们知道TCP通过一个timer采样了RTT并计算RTO，但是，如果网络上的延时突然增加，那么，TCP对这个事做出的应对只有重传数据，但是，重传会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，于是，这个情况就会进入恶性循环被不断地放大。试想一下，如果一个网络内有成千上万的TCP连接都这么行事，那么马上就会形成“网络风暴”，TCP这个协议就会拖垮整个网络。这是一个灾难。

所以，TCP不能忽略网络上发生的事情，而无脑地一个劲地重发数据，对网络造成更大的伤害。对此TCP的设计理念是：[red]#TCP不是一个自私的协议，当拥塞发生的时候，要做自我牺牲。就像交通阻塞一样，每个车都应该把路让出来，而不要再去抢路了。#

关于拥塞控制的论文请参看《 https://ee.lbl.gov/papers/congavoid.pdf[Congestion Avoidance and Control]》(PDF)

拥塞控制主要是四个算法：[yellow]*1）慢启动，2）拥塞避免，3）拥塞发生，4）快速恢复*。这四个算法不是一天都搞出来的，这个四个算法的发展经历了很多时间，到今天都还在优化中。

备注:

- 1988年，TCP-Tahoe 提出了1）慢启动，2）拥塞避免，3）拥塞发生时的快速重传
- 1990年，TCP Reno 在Tahoe的基础上增加了4）快速恢复

===== 慢热启动算法 – Slow Start
首先，我们来看一下TCP的慢热启动。慢启动的意思是，刚刚加入网络的连接，一点一点地提速，不要一上来就像那些特权车一样霸道地把路占满。

慢启动的算法如下(cwnd全称Congestion Window)：

 - 1）连接建好的开始先初始化cwnd = 1，表明可以传一个MSS大小的数据。
 - 2）每当收到一个ACK，cwnd++; 呈线性上升
 - 3）每当过了一个RTT，cwnd = cwnd*2; 呈指数让升
 - 4）还有一个ssthresh（slow start threshold），是一个上限，当cwnd >= ssthresh时，就会进入“拥塞避免算法”（后面会说这个算法）

所以，我们可以看到，如果网速很快的话，ACK也会返回得快，RTT也会短，那么，这个慢启动就一点也不慢。下图说明了这个过程。

image::image-2023-06-15-15-57-12-578.png[]

这里，我需要提一下的是一篇Google的论文《 https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/36640.pdf[An Argument for Increasing TCP’s Initial Congestion Window]》Linux 3.0后采用了这篇论文的建议——把cwnd 初始化成了 10个MSS。 而Linux 3.0以前，比如2.6，Linux采用了RFC3390，cwnd是跟MSS的值来变的，如果MSS< 1095，则cwnd = 4；如果MSS>2190，则cwnd=2；其它情况下，则是3。

===== 拥塞避免算法 – Congestion Avoidance
前面说过，还有一个ssthresh（slow start threshold），是一个上限，当cwnd >= ssthresh时，就会进入“拥塞避免算法”。一般来说ssthresh的值是65535，单位是字节，当cwnd达到这个值时后，算法如下：

- 1）收到一个ACK时，cwnd = cwnd + 1/cwnd

- 2）当每过一个RTT时，cwnd = cwnd + 1

这样就可以避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。很明显，是一个线性上升的算法。

===== 拥塞状态时的算法
前面我们说过，当丢包的时候，会有两种情况：

- 1）等到RTO超时，重传数据包。TCP认为这种情况太糟糕，反应也很强烈。
    1. sshthresh =  cwnd /2
    2. cwnd 重置为 1
    3. 进入慢启动过程
- 2）Fast Retransmit算法，也就是在收到3个duplicate ACK时就开启重传，而不用等到RTO超时。
    1. TCP Tahoe的实现和RTO超时一样。
    2. TCP Reno的实现是：
        . cwnd = cwnd /2
        . sshthresh = cwnd
        . 进入快速恢复算法——Fast Recovery

上面我们可以看到RTO超时后，sshthresh会变成cwnd的一半，这意味着，如果cwnd<=sshthresh时出现的丢包，那么TCP的sshthresh就会减了一半，然后等cwnd又很快地以指数级增涨爬到这个地方时，就会成慢慢的线性增涨。我们可以看到，TCP是怎么通过这种强烈地震荡快速而小心得找到网站流量的平衡点的。

===== 快速恢复算法 – Fast Recovery

*TCP Reno*

这个算法定义在RFC5681。快速重传和快速恢复算法一般同时使用。快速恢复算法是认为，你还有3个Duplicated Acks说明网络也不那么糟糕，所以没有必要像RTO超时那么强烈。 注意，正如前面所说，进入Fast Recovery之前，cwnd 和 sshthresh已被更新：

- cwnd = cwnd /2
- sshthresh = cwnd

然后，真正的Fast Recovery算法如下：

- cwnd = sshthresh  + 3 * MSS （3的意思是确认有3个数据包被收到了）
- 重传Duplicated ACKs指定的数据包
- 如果再收到 duplicated Acks，那么cwnd = cwnd +1
- 如果收到了新的Ack，那么，cwnd = sshthresh ，然后就进入了拥塞避免的算法了。

如果你仔细思考一下上面的这个算法，你就会知道，上面这个算法也有问题，那就是——它依赖于3个重复的Acks。注意，3个重复的Acks并不代表只丢了一个数据包，很有可能是丢了好多包。但这个算法只会重传一个，而剩下的那些包只能等到RTO超时，于是，进入了恶梦模式——超时一个窗口就减半一下，多个超时会超成TCP的传输速度呈级数下降，而且也不会触发Fast Recovery算法了。

通常来说，正如我们前面所说的，SACK或D-SACK的方法可以让Fast Recovery或Sender在做决定时更聪明一些，但是并不是所有的TCP的实现都支持SACK（SACK需要两端都支持），所以，需要一个没有SACK的解决方案。而通过SACK进行拥塞控制的算法是FACK（后面会讲）

*TCP New Reno*

于是，1995年，TCP New Reno（参见 RFC 6582 ）算法提出来，主要就是在没有SACK的支持下改进Fast Recovery算法的——

- 当sender这边收到了3个Duplicated Acks，进入Fast Retransimit模式，开发重传重复Acks指示的那个包。如果只有这一个包丢了，那么，重传这个包后回来的Ack会把整个已经被sender传输出去的数据ack回来。如果没有的话，说明有多个包丢了。我们叫这个ACK为Partial ACK。
- 一旦Sender这边发现了Partial ACK出现，那么，sender就可以推理出来有多个包被丢了，于是乎继续重传sliding window里未被ack的第一个包。直到再也收不到了Partial Ack，才真正结束Fast Recovery这个过程

我们可以看到，这个“Fast Recovery的变更”是一个非常激进的玩法，他同时延长了Fast Retransmit和Fast Recovery的过程。

*算法示意图*

下面我们来看一个简单的图示以同时看一下上面的各种算法的样子：

image::image-2023-06-15-15-58-00-020.png[]

参考： https://www.coddicted.com/tcp-congestion-control-algorithm/[tcp congestion-control-algorithm]

===== FACK算法
FACK全称Forward Acknowledgment 算法，论文地址在这里 http://conferences.sigcomm.org/sigcomm/1996/papers/mathis.pdf[Forward Acknowledgement: Refining TCP Congestion Control] 这个算法是其于SACK的，前面我们说过SACK是使用了TCP扩展字段Ack了有哪些数据已经收到，哪些数据没有收到，他比Fast Retransmit的3 个duplicated acks好处在于，前者只知道有包丢了，不知道是一个还是多个，而SACK可以准确的知道有哪些包丢了。 所以，SACK可以让发送端这边在重传过程中，把那些丢掉的包重传，而不是一个一个的传，但这样的一来，如果重传的包数据比较多的话，又会导致本来就很忙的网络就更忙了。所以，FACK用来做重传过程中的拥塞流控。

我们可以看到如果没有FACK在，那么在丢包比较多的情况下，原来保守的算法会低估了需要使用的window的大小，而需要几个RTT的时间才会完成恢复，而FACK会比较激进地来干这事。 但是，FACK如果在一个网络包会被 reordering的网络里会有很大的问题。

==== 其它拥塞控制算法简介

===== TCP Vegas 拥塞控制算法
这个算法1994年被提出，它主要对TCP Reno 做了些修改。这个算法通过对RTT的非常重的监控来计算一个基准RTT。然后通过这个基准RTT来估计当前的网络实际带宽，如果实际带宽比我们的期望的带宽要小或是要多的活，那么就开始线性地减少或增加cwnd的大小。如果这个计算出来的RTT大于了Timeout后，那么，不等ack超时就直接重传。（Vegas 的核心思想是用RTT的值来影响拥塞窗口，而不是通过丢包） 这个算法的论文是《 http://www.cs.cmu.edu/~srini/15-744/F02/readings/BP95.pdf[TCP Vegas: End to End Congestion Avoidance on a Global Internet]》这篇论文给了Vegas和 New Reno的对比：

image::image-2023-06-15-15-58-40-969.png[]

关于这个算法实现，你可以参看Linux源码：/net/ipv4/tcp_vegas.h， /net/ipv4/tcp_vegas.c

===== HSTCP(High Speed TCP) 算法
这个算法来自RFC 3649（Wikipedia词条）。其对最基础的算法进行了更改，他使得Congestion Window涨得快，减得慢。其中：

拥塞避免时的窗口增长方式： cwnd = cwnd + α(cwnd) / cwnd
丢包后窗口下降方式：cwnd = (1- β(cwnd))*cwnd
注：α(cwnd)和β(cwnd)都是函数，如果你要让他们和标准的TCP一样，那么让α(cwnd)=1，β(cwnd)=0.5就可以了。 对于α(cwnd)和β(cwnd)的值是个动态的变换的东西。 关于这个算法的实现，你可以参看Linux源码：/net/ipv4/tcp_highspeed.c

*TCP BIC 算法*

2004年，推出出BIC算法。现在你还可以查得到相关的新闻《Google：美科学家研发BIC-TCP协议 速度是DSL六千倍》 BIC全称Binary Increase Congestion control，在Linux 2.6.8中是默认拥塞控制算法。BIC的发明者发这么多的拥塞控制算法都在努力找一个合适的cwnd – Congestion Window，而且BIC-TCP的提出者们看穿了事情的本质，其实这就是一个搜索的过程，所以BIC这个算法主要用的是Binary Search——二分查找来干这个事。 关于这个算法实现，你可以参看Linux源码：/net/ipv4/tcp_bic.c

*TCP WestWood算法*

westwood采用和Reno相同的慢启动算法、拥塞避免算法。westwood的主要改进方面：在发送端做带宽估计，当探测到丢包时，根据带宽值来设置拥塞窗口、慢启动阈值。 那么，这个算法是怎么测量带宽的？每个RTT时间，会测量一次带宽，测量带宽的公式很简单，就是这段RTT内成功被ack了多少字节。因为，这个带宽和用RTT计算RTO一样，也是需要从每个样本来平滑到一个值的——也是用一个加权移平均的公式。 另外，我们知道，如果一个网络的带宽是每秒可以发送X个字节，而RTT是一个数据发出去后确认需要的时候，所以，X * RTT应该是我们缓冲区大小。所以，在这个算法中，ssthresh的值就是est_BD * min-RTT(最小的RTT值)，如果丢包是Duplicated ACKs引起的，那么如果cwnd > ssthresh，则 cwin = ssthresh。如果是RTO引起的，cwnd = 1，进入慢启动。   关于这个算法实现，你可以参看Linux源码： /net/ipv4/tcp_westwood.c

===== 其它
更多的算法，你可以从Wikipedia的 TCP Congestion Avoidance Algorithm 词条中找到相关的线索。

==== 参考： +
https://github.com/InterviewMap/CS-Interview-Knowledge-Map/blob/master/Network/Network_en.md[Network] +
https://www.filibeto.org/sun/lib/networking/tuning/ttcp.html[tcp transaction] +
https://blog.liu-kevin.com/2021/01/04/linuxzhi-wang-luo/[tcp调优] +
https://www.cnblogs.com/LOVE0612/p/15043947.html[tcp相关算法] +
https://en.wikipedia.org/wiki/SYN_flood[syn flood攻击] +
https://www.rfc-editor.org/rfc/rfc4987[rfc4987] +
https://www.sobyte.net/post/2021-12/whys-the-design-tcp-time-wait/[为什么TCP有一个time wait状态] +
https://www.rfc-editor.org/rfc/pdfrfc/rfc793.txt.pdf[tcp] +
https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt[linux ip] +
https://tcpcc.systemsapproach.org/tcp_ip.html[tcp设计说明没事可以进行精度] +
https://www.ecb.torontomu.ca/~courses/ee8211/L5_CL_Congestion.pdf[Congestion]


=== TCP连接建立过程

- 为什么服务端程序需要先listen一下
- 半连接和全连接队列长度如何确定

> tcp服务端在处理三次握手的时候需要有半连接和全连接队列配合完成，那么这两个队列在内核中是什么样子？如何想修改他们的长度如何完成？

- cannot assign requested address这个报错你知道是怎么回事吗？该如何解决
- 一个客户端端口可以同时用在两个连接上吗？

> 假设一个客户端某个端口号已经和某个服务端建立连接了，那么再想和其他服务端建立连接这个端口还能用吗？

- 服务端的全连接和半连接全满了会怎么样？

> 如果服务端连接请求过于频繁，导致全连接和半连接全部都满了会怎样？会不会导致线上问题？会不会导致连接队列溢出，如果有，怎么办？怎样才能解决？

- 创建新连接时，新连接的socket内核对象是什么时候创建的？
- 建立一条tcp连接需要消耗多长时间？
- 服务器负载正常但是CPU被打到底(100%)了是怎么回事？


.网络协议栈
image::image-2023-06-07-15-23-19-171.png[网络协议栈, , align="center"]


=== 深入理解listen

在开启服务之前总是会调用一下listen系统接口，那这个底层干了啥？为啥服务端需要调用客户端不需要调用？

==== 系统函数

socket函数的定义在net/socket.c文件中。具体实现如下：

.file net/socket.c
[source, cpp]
----

/*
 *	Perform a listen. Basically, we allow the protocol to do anything
 *	necessary for a listen, and if that works, we mark the socket as
 *	ready for listening.
 */

SYSCALL_DEFINE2(listen, int, fd, int, backlog)
{
	struct socket *sock;
	int err, fput_needed;
	int somaxconn;

	sock = sockfd_lookup_light(fd, &err, &fput_needed);
	if (sock) {
	    // 获取内核参数 net.core.somaxconn - 内核配置允许最大的连接队列个数
		somaxconn = sock_net(sock->sk)->core.sysctl_somaxconn;
		if ((unsigned int)backlog > somaxconn)
			backlog = somaxconn;

		err = security_socket_listen(sock, backlog);
		if (!err)  // 使用协议栈注册进来的listen函数
			err = sock->ops->listen(sock, backlog);

		fput_light(sock->file, fput_needed);
	}
	return err;
}
----

用户使用的socket文件描述符，只是一个整数，内核是没有办法使用的，所以实际使用中需要根据用户提供的整数查找出内核使用的socket对象。




































参考：

https://mp.weixin.qq.com/s?__biz=MzA3NjY2NzY1MA==&mid=2649740393&idx=1&sn=b048e8e068052549af0c44cb678a7140&chksm=8746ba04b0313312fe87e346c0c68d235a8e81b31de1453392427af6384e612ad44713627eb0&scene=27[eBPF内核协议栈背负太多的历史包袱]
https://mp.weixin.qq.com/s/uWRg1fhHZh_ttd2NUsAh9w[XDPF/eBPF]
https://mp.weixin.qq.com/s/TwKOwg2RFBYKZF160Zw7lA[一个奇葩的网络问题]

https://coolshell.cn/articles/11564.html[tcp那些事]









